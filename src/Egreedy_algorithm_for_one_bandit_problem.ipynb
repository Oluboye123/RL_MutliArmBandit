{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "USING EPSILON GREEDY POILICY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedyAgent(object):\n",
    "    def __init__(self, env, max_iterations=200, epsilon=0.1, decay = 1, decay_interval = 50):\n",
    "        self.env = env \n",
    "        self.iterations = max_iterations\n",
    "        self.epsilon = epsilon \n",
    "        self.decay = decay \n",
    "        self.decay_interval = decay_interval\n",
    "\n",
    "    def execute(self):\n",
    "        q_values = np.zeros(self.env.k_arms)\n",
    "        arm_rewards = np.zeros(self.env.k_arms)\n",
    "        arm_counts = np.zeros(self.env.k_arms)\n",
    "        reward_eps_optimal = np.empty(self.env.k_arms)\n",
    "        rewards = []\n",
    "        cum_rewards = []\n",
    "       \n",
    "        \n",
    "        for i in range(1, self.iterations + 1):\n",
    "            arm = np.random.choice(self.env.k_arms) if np.random.random() < self.epsilon else np.argmax(q_values)\n",
    "            reward = self.env.pull_arm(arm)\n",
    "\n",
    "            arm_rewards[arm] += reward\n",
    "            arm_counts[arm] += 1\n",
    "            reward_eps_optimal[arm] += (reward)\n",
    "            q_values[arm] = arm_rewards[arm]/arm_counts[arm]\n",
    "\n",
    "            rewards.append(reward)\n",
    "            cum_rewards.append(sum(rewards)/ len(rewards))\n",
    "            if i % self.decay_interval == 0:\n",
    "                self.epsilon = self.epsilon * self.decay\n",
    "            #print(\"iteration \", i ,\" pulled arm \",arm ,\" with reward \", reward)    \n",
    "        return {\"arms\": arm_counts, \"rewards\": rewards, \"cum_rewards\": cum_rewards, \"reward_eps\" : reward_eps_optimal}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "USING OPTIMAL INITIALISATION POLICY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimisticAgent(object):\n",
    "    def __init__(self, env, initial_q=10, initial_visits=100, max_iterations=2000):\n",
    "        self.env = env\n",
    "        self.initial_q = initial_q\n",
    "        self.initial_visits = initial_visits\n",
    "        self.iterations = max_iterations \n",
    "\n",
    "    def execute(self):\n",
    "        q_values = np.ones(self.env.k_arms) * self.initial_q\n",
    "        arm_counts = np.ones(self.env.k_arms) * self.initial_visits\n",
    "        arm_rewards = np.zeros(self.env.k_arms)\n",
    "        reward_eps_optimal = np.empty(self.env.k_arms)\n",
    "        rewards = []\n",
    "        cum_rewards = []\n",
    "\n",
    "        for i in range(1, self.iterations):\n",
    "            arm = np.argmax(q_values)\n",
    "            reward = self.env.pull_arm(arm)\n",
    "\n",
    "            arm_rewards[arm] += reward\n",
    "            arm_counts[arm] += 1\n",
    "            reward_eps_optimal[arm] += (reward)\n",
    "            q_values[arm] = arm_rewards[arm] / arm_counts[arm]\n",
    "\n",
    "            rewards.append(reward)\n",
    "            cum_rewards.append(sum(rewards)/len(rewards))\n",
    "\n",
    "        return {\"arms\": arm_counts, \"rewards\": rewards, \"cum_rewards\": cum_rewards, \"reward_eps\" : reward_eps_optimal}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "bb33f6d328f18c07440802b8c66874c52744b86bff5cfe8eb1d71afeb55a2150"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
